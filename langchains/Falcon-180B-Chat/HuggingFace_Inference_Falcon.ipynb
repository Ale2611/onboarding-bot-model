{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "15908f0e",
      "metadata": {
        "id": "15908f0e"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "94f0ccef",
      "metadata": {
        "id": "94f0ccef",
        "outputId": "4798f9ef-5d0d-40c7-a0f3-bbe264102e42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-02-01 11:55:15 - Created default config file at c:\\git\\onboarding-bot-model\\models\\.chainlit\\config.toml\n",
            "2024-02-01 11:55:15 - Created default translation directory at c:\\git\\onboarding-bot-model\\models\\.chainlit\\translations\n",
            "2024-02-01 11:55:15 - Created default translation file at c:\\git\\onboarding-bot-model\\models\\.chainlit\\translations\\en-US.json\n",
            "2024-02-01 11:55:15 - Created default translation file at c:\\git\\onboarding-bot-model\\models\\.chainlit\\translations\\pt-BR.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import chainlit as cl\n",
        "from langchain import HuggingFaceHub, PromptTemplate, LLMChain\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58b927f4",
      "metadata": {
        "id": "58b927f4"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9837afb7",
      "metadata": {
        "id": "9837afb7"
      },
      "outputs": [],
      "source": [
        "model_id = 'tiiuae/falcon-7b-instruct'\n",
        "\n",
        "falcon_llm = HuggingFaceHub(huggingfacehub_api_token=os.environ['API_KEY'],\n",
        "                            repo_id=model_id,\n",
        "                            model_kwargs={\"temperature\":0.8,\"max_new_tokens\":2000})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b37f5f57",
      "metadata": {
        "id": "b37f5f57"
      },
      "source": [
        "## Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b53f6c18",
      "metadata": {
        "id": "b53f6c18"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"Sandiago21/falcon-7b-prompt-answering\"\n",
        "# MODEL_NAME = \".\"\n",
        "# BASE_MODEL = \"tiiuae/falcon-7b\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec8111a9",
      "metadata": {
        "id": "ec8111a9"
      },
      "source": [
        "## Load Model & Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d6c0966c",
      "metadata": {
        "id": "d6c0966c",
        "outputId": "87b1c939-5724-48df-b307-784061b80dd3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'tiiuae/falcon-7b'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
        "config.base_model_name_or_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ebd614a3",
      "metadata": {
        "id": "ebd614a3",
        "outputId": "d147ccd3-e9f5-43d2-cb2e-1e7c3017e797"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'tiiuae/falcon-7b'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config.base_model_name_or_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1cb5103c",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "cfb5a01e129d48e7871a15fda0511006"
          ]
        },
        "id": "1cb5103c",
        "outputId": "3fac2f0f-ea8e-4b76-a03b-5a76ed2dd495"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "No GPU found. A GPU is needed for quantization.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m compute_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[0;32m      4\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mcompute_dtype,\n\u001b[0;32m      7\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      8\u001b[0m )\n\u001b[1;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     11\u001b[0m     config\u001b[38;5;241m.\u001b[39mbase_model_name_or_path,\n\u001b[0;32m     12\u001b[0m     quantization_config\u001b[38;5;241m=\u001b[39mbnb_config,\n\u001b[0;32m     13\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     14\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_NAME)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\auto\\auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    560\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    562\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    563\u001b[0m     )\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:3030\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_8bit \u001b[38;5;129;01mor\u001b[39;00m load_in_4bit:\n\u001b[0;32m   3029\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m-> 3030\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo GPU found. A GPU is needed for quantization.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3031\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[0;32m   3032\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   3033\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3034\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3035\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `pip install bitsandbytes`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3036\u001b[0m         )\n",
            "\u001b[1;31mRuntimeError\u001b[0m: No GPU found. A GPU is needed for quantization."
          ]
        }
      ],
      "source": [
        "compute_dtype = getattr(torch, \"float16\")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "926651de",
      "metadata": {
        "id": "926651de"
      },
      "outputs": [],
      "source": [
        "# model.eval()\n",
        "# if torch.__version__ >= \"2\":\n",
        "#     model = torch.compile(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d265647e",
      "metadata": {
        "id": "d265647e"
      },
      "source": [
        "## Generation Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "10372ae3",
      "metadata": {
        "id": "10372ae3"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generation_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m      2\u001b[0m generation_config\u001b[38;5;241m.\u001b[39mtop_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m\n\u001b[0;32m      3\u001b[0m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "generation_config = model.generation_config\n",
        "generation_config.top_p = 0.7\n",
        "generation_config.num_return_sequences = 1\n",
        "generation_config.max_new_tokens = 32\n",
        "generation_config.use_cache = False\n",
        "generation_config.pad_token_id = tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2ac4b78",
      "metadata": {
        "id": "e2ac4b78"
      },
      "source": [
        "## Examples with Base (tiiuae/falcon-7b) model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f6e7df1",
      "metadata": {
        "id": "1f6e7df1"
      },
      "source": [
        "### Example 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a84a4f9e",
      "metadata": {
        "id": "a84a4f9e",
        "outputId": "18383b62-668d-4e50-a363-9a74fce906f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating...\n",
            "<human>: Como cocinar supa de pescado?\n",
            "<assistant>: ¿Qué quiere decir \"supa de pescado\"?\n",
            "<human>: ¿Como cocinar supa de pescado?\n",
            "<\n",
            "CPU times: user 9.68 s, sys: 188 ms, total: 9.87 s\n",
            "Wall time: 9.93 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "PROMPT = \"\"\"\n",
        "<human>: Como cocinar supa de pescado?\n",
        "<assistant>:\n",
        "\"\"\".strip()\n",
        "\n",
        "inputs = tokenizer(\n",
        "    PROMPT,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "input_ids = inputs[\"input_ids\"].cuda()\n",
        "attention_mask = inputs[\"attention_mask\"].cuda()\n",
        "\n",
        "print(\"Generating...\")\n",
        "with torch.no_grad():\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8143ca1f",
      "metadata": {
        "id": "8143ca1f"
      },
      "source": [
        "### Example 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65117ac7",
      "metadata": {
        "id": "65117ac7",
        "outputId": "8321a4fa-623b-463b-d58d-fa7a30fe69e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating...\n",
            "<human>: What is the capital city of Greece and with which countries does Greece border?\n",
            "<assistant>: The capital city of Greece is Athens. Greece borders Albania, Bulgaria, Macedonia, and Turkey.\n",
            "<human>: What is the capital city of Albania and with\n",
            "CPU times: user 8.81 s, sys: 0 ns, total: 8.81 s\n",
            "Wall time: 8.8 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "PROMPT = \"\"\"\n",
        "<human>: What is the capital city of Greece and with which countries does Greece border?\n",
        "<assistant>:\n",
        "\"\"\".strip()\n",
        "\n",
        "inputs = tokenizer(\n",
        "    PROMPT,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "input_ids = inputs[\"input_ids\"].cuda()\n",
        "attention_mask = inputs[\"attention_mask\"].cuda()\n",
        "\n",
        "print(\"Generating...\")\n",
        "with torch.no_grad():\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "447f75f9",
      "metadata": {
        "id": "447f75f9"
      },
      "source": [
        "### Example 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ff7a5e5",
      "metadata": {
        "id": "2ff7a5e5",
        "outputId": "d1e6be6b-6a3b-4222-f424-65a6a2195fb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating...\n",
            "<human>: Ποιά είναι η μεγαλύτερη πόλη της Ελλάδας?\n",
            "<assistant>: Ποιά είναι η μεγαλύτερη πόλη τ\n",
            "CPU times: user 9.29 s, sys: 0 ns, total: 9.29 s\n",
            "Wall time: 9.29 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "PROMPT = \"\"\"\n",
        "<human>: Ποιά είναι η μεγαλύτερη πόλη της Ελλάδας?\n",
        "<assistant>:\n",
        "\"\"\".strip()\n",
        "\n",
        "inputs = tokenizer(\n",
        "    PROMPT,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "input_ids = inputs[\"input_ids\"].cuda()\n",
        "attention_mask = inputs[\"attention_mask\"].cuda()\n",
        "\n",
        "print(\"Generating...\")\n",
        "with torch.no_grad():\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0f1fc51",
      "metadata": {
        "id": "c0f1fc51"
      },
      "source": [
        "### Example 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4073cb6d",
      "metadata": {
        "id": "4073cb6d",
        "outputId": "0fe124e0-fee8-43f7-8ecb-15845e2a2d93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating...\n",
            "<human>: I have two oranges and 3 apples. How many fruits do I have in total?\n",
            "<assistant>: 5\n",
            "<human>: 5?\n",
            "<assistant>: Yes\n",
            "<human>: I have 2 oranges and 3 apples. How many fruits\n",
            "CPU times: user 8.85 s, sys: 0 ns, total: 8.85 s\n",
            "Wall time: 8.86 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "PROMPT = \"\"\"\n",
        "<human>: I have two oranges and 3 apples. How many fruits do I have in total?\n",
        "<assistant>:\n",
        "\"\"\".strip()\n",
        "\n",
        "inputs = tokenizer(\n",
        "    PROMPT,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "input_ids = inputs[\"input_ids\"].cuda()\n",
        "attention_mask = inputs[\"attention_mask\"].cuda()\n",
        "\n",
        "print(\"Generating...\")\n",
        "with torch.no_grad():\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        generation_config=generation_config,\n",
        ")\n",
        "\n",
        "response = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e2d35b3",
      "metadata": {
        "id": "2e2d35b3"
      },
      "source": [
        "## Examples with Fine-Tuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df08ac5a",
      "metadata": {
        "id": "df08ac5a"
      },
      "source": [
        "## Let's Load the Fine-Tuned version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cba7db1",
      "metadata": {
        "id": "9cba7db1"
      },
      "outputs": [],
      "source": [
        "model = PeftModel.from_pretrained(model, MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bc70c31",
      "metadata": {
        "id": "5bc70c31"
      },
      "source": [
        "### Example 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af3a477a",
      "metadata": {
        "id": "af3a477a",
        "outputId": "53c3d601-ebdb-4a36-c693-346b1c130dea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating...\n",
            "<human>: Como cocinar supa de pescado?\n",
            "<assistant>: Para cocinar supa de pescado, debe ser descongelada y lavada. Después, debe ser cortada en trozos pequeños y\n",
            "CPU times: user 9.34 s, sys: 3.68 ms, total: 9.35 s\n",
            "Wall time: 9.34 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "PROMPT = \"\"\"\n",
        "<human>: Como cocinar supa de pescado?\n",
        "<assistant>:\n",
        "\"\"\".strip()\n",
        "\n",
        "inputs = tokenizer(\n",
        "    PROMPT,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "input_ids = inputs[\"input_ids\"].cuda()\n",
        "attention_mask = inputs[\"attention_mask\"].cuda()\n",
        "\n",
        "print(\"Generating...\")\n",
        "with torch.no_grad():\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "622b3c0a",
      "metadata": {
        "id": "622b3c0a"
      },
      "source": [
        "### Example 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eab112ae",
      "metadata": {
        "id": "eab112ae",
        "outputId": "03a82286-2b0b-4d9c-b650-52636e9d5b66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating...\n",
            "<human>: What is the capital city of Greece and with which countries does Greece border?\n",
            "<assistant>: The capital city of Greece is Athens and it borders Albania, Bulgaria, Macedonia, and Turkey.\n",
            "<human>: What is the capital city of Greece and with\n",
            "CPU times: user 9.67 s, sys: 0 ns, total: 9.67 s\n",
            "Wall time: 9.66 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "PROMPT = \"\"\"\n",
        "<human>: What is the capital city of Greece and with which countries does Greece border?\n",
        "<assistant>:\n",
        "\"\"\".strip()\n",
        "\n",
        "inputs = tokenizer(\n",
        "    PROMPT,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "input_ids = inputs[\"input_ids\"].cuda()\n",
        "attention_mask = inputs[\"attention_mask\"].cuda()\n",
        "\n",
        "print(\"Generating...\")\n",
        "with torch.no_grad():\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb0e6d9e",
      "metadata": {
        "id": "fb0e6d9e"
      },
      "source": [
        "### Example 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df571d56",
      "metadata": {
        "id": "df571d56",
        "outputId": "5e829c9c-118c-4c88-bb8c-6bdeb089aee9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating...\n",
            "<human>: Ποιά είναι η μεγαλύτερη πόλη της Ελλάδας?\n",
            "<assistant>: Το Αθήνα είναι το πλήρες κόσ\n",
            "CPU times: user 9.46 s, sys: 0 ns, total: 9.46 s\n",
            "Wall time: 9.45 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "PROMPT = \"\"\"\n",
        "<human>: Ποιά είναι η μεγαλύτερη πόλη της Ελλάδας?\n",
        "<assistant>:\n",
        "\"\"\".strip()\n",
        "\n",
        "inputs = tokenizer(\n",
        "    PROMPT,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "input_ids = inputs[\"input_ids\"].cuda()\n",
        "attention_mask = inputs[\"attention_mask\"].cuda()\n",
        "\n",
        "print(\"Generating...\")\n",
        "with torch.no_grad():\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d3aa375",
      "metadata": {
        "id": "8d3aa375"
      },
      "source": [
        "### Example 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4975198b",
      "metadata": {
        "id": "4975198b",
        "outputId": "e1fdfc7b-c896-47b8-9361-e20a6d5a7931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating...\n",
            "<human>: I have two oranges and 3 apples. How many fruits do I have in total?\n",
            "<assistant>: You have 2 oranges and 3 apples. You have 5 fruits in total. You can also use the following formula to calculate the number of fruits you\n",
            "CPU times: user 8.93 s, sys: 0 ns, total: 8.93 s\n",
            "Wall time: 8.92 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "PROMPT = \"\"\"\n",
        "<human>: I have two oranges and 3 apples. How many fruits do I have in total?\n",
        "<assistant>:\n",
        "\"\"\".strip()\n",
        "\n",
        "inputs = tokenizer(\n",
        "    PROMPT,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "input_ids = inputs[\"input_ids\"].cuda()\n",
        "attention_mask = inputs[\"attention_mask\"].cuda()\n",
        "\n",
        "print(\"Generating...\")\n",
        "with torch.no_grad():\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "045c1d46",
      "metadata": {
        "id": "045c1d46"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ctransformers'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mctransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m llm \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTheBloke/zephyr-7B-beta-GGUF\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzephyr-7b-beta.Q4_K_M.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral\u001b[39m\u001b[38;5;124m\"\u001b[39m, gpu_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ctransformers'"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a87f4d5e",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59372346",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "47a1b0a6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fbce797d9ccf403d82ced326cf4be4fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0205a34d9b364285993e940e637dbe37",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "from ctransformers import AutoModelForCausalLM\n",
        "\n",
        "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
        "llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/zephyr-7B-beta-GGUF\", model_file=\"zephyr-7b-beta.Q4_K_M.gguf\", model_type=\"mistral\", gpu_layers=50)\n",
        "\n",
        "print(llm(\"AI is going to\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4daab46",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0240a8e8d4a34e2bb4ef1ac9df96d262",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78d7a8b266c546b79a4b0173ed2179e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "from ctransformers import AutoModelForCausalLM\n",
        "\n",
        "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
        "llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/zephyr-7B-beta-GGUF\", model_file=\"zephyr-7b-beta.Q4_K_M.gguf\", model_type=\"mistral\", gpu_layers=50)\n",
        "\n",
        "print(llm(\"AI is going to\"))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
