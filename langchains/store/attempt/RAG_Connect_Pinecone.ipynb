{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c8fde4c-9222-4265-b72b-8d7693520250",
   "metadata": {},
   "source": [
    "# Using Retrieval-Augmented Generation to Search Research Notes Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e302e1c-4c18-4c44-87fd-ba935c3a0853",
   "metadata": {},
   "source": [
    "Retrieval-augmented generation, or _RAG_, is a technique used with large language models to provide additional context without fine-tuning or retraining. It enhances the ability of language models to provide factual responses, which is a limitation of classical setups.\n",
    "\n",
    "The goal of this project is to build a question-answering bot for movie-related questions. To achieve this, we will use RAG to provide factual information to the language model. We will upload movie descriptions to a vector database and use it to search for relevant context for the language model.\n",
    "\n",
    "We will be using the following tools and models:\n",
    "- [OpenAI](https://openai.com)'s `gpt-3.5-turbo` model for prompt completions\n",
    "- OpenAI's `text-embedding-ada-002` model to create vector embeddings\n",
    "- [Pinecone](https://www.pinecone.io/) as the vector database to store the embeddings\n",
    "- [langchain](https://www.langchain.com/) as the tool to interact with OpenAI and Pinecone\n",
    "\n",
    "The dataset used for this project is sourced from the Kaggle dataset [IMDb Movies/Shows with Descriptions](https://www.kaggle.com/datasets/ishikajohari/imdb-data-with-descriptions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231d2c6-275e-4399-b7cd-84e112831d08",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785d7fac-edb2-482f-be2b-c63dc2882103",
   "metadata": {},
   "source": [
    "To get started with this project, you'll need a developer account for OpenAI and Pinecone. Follow the steps in the [getting-started.ipynb](https://app.datacamp.com/workspace/w/f1d996aa-0aaa-47e3-bd61-2b5b5a0fa558/edit/getting-started.ipynb) notebook to create an API key and store it in Workspace.\n",
    "\n",
    "For this project, we will assume that you have already set the `OPENAI_API_KEY` and `PINECONE_API_KEY` environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9274661-8d8c-4cc5-901e-5fc497866b89",
   "metadata": {},
   "source": [
    "## Task 0: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf847fd-f8f8-49f6-9b43-0eb098239072",
   "metadata": {},
   "source": [
    "To perform this analysis, we need to install the following packages:\n",
    "\n",
    "- `openai`: for interacting with OpenAI\n",
    "- `pinecone-client`: for interacting with Pinecone\n",
    "- `tiktoken`: a string encoder that generates tokens used by OpenAI. It is useful for estimating the number of tokens used.\n",
    "- `langchain`: the toolchain used to interact with OpenAI and Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a9caca-70fd-4ac0-aa15-1bee55c456d3",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fcd794-b29c-4010-8be0-651a452b2044",
   "metadata": {},
   "source": [
    "Run the cell below to install the corresponding packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14de5322-82bf-475e-9e08-e20a5bc8b9d7",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 8864,
    "lastExecutedAt": 1695209311314,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "%%capture\n# Below we installed specific versions of the packages\n# Feel free to experiment with different versions\n# However, the workspace below is only tested with these specific versions\n!pip install pinecone-client==2.2.2 openai==0.28.0 tiktoken==0.5.1 langchain==0.0.291",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Below we installed specific versions of the packages\n",
    "# Feel free to experiment with different versions\n",
    "# However, the workspace below is only tested with these specific versions\n",
    "!pip install pinecone-client==2.2.2 openai==0.28.0 tiktoken==0.5.1 langchain==0.0.291"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "03110dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\ahmed\\\\CodeLaunchers\\\\onboarding-bot-model\\\\langchains\\\\GitHub'"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5531d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up secrets file\n",
    "\n",
    "def load_secrets(file_path):\n",
    "    secrets = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.strip() and not line.startswith('#'):  # Exclude empty and comment lines\n",
    "                key, value = line.strip().split('=', 1)\n",
    "                secrets[key] = value\n",
    "    return secrets\n",
    "\n",
    "# Load your secrets\n",
    "secrets_file_path = 'c:\\\\Users\\\\ahmed\\\\Downloads\\\\chatbot_secrets.env'  # Update this to your file's path\n",
    "secrets = load_secrets(secrets_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a214b50c-8ee1-4a1d-a940-4ea1d16da052",
   "metadata": {},
   "source": [
    "## Task 1: Import the Research Notes Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d97255b-a124-41a3-b389-2be5b409eff0",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 124,
    "lastExecutedAt": 1695209311439,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import pandas as pd\nimport pandas as pd\n\n# Import IMBD.csv and transform to create the movies dataframe\nmovies = pd.read_csv(\"IMDB.csv\")\nmovies = movies.rename(columns = {\n    \"primaryTitle\": \"movie_title\",\n    \"Description\": \"movie_description\",\n})\nmovies[\"source\"] = \"https://www.imdb.com/title/\" + movies[\"tconst\"]\nmovies = movies.loc[movies[\"titleType\"] == \"movie\"]\nmovies = movies[[\"movie_title\", \"movie_description\", \"source\", \"genres\"]]\n\n# Show the head of movies\nmovies.head()",
    "outputsMetadata": {
     "0": {
      "height": 193,
      "type": "dataFrame"
     }
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "readme_files = pd.read_csv(\"readmes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c756883a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['url', 'repo_name', 'readme_text'], dtype='object')"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme_files.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fab0bf99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                                 url               repo_name  \\\n0  https://raw.githubusercontent.com/dev-launcher...  dev-launchers-platform   \n1  https://raw.githubusercontent.com/dev-launcher...              auth-proxy   \n2  https://raw.githubusercontent.com/dev-launcher...    onboarding-bot-model   \n3  https://raw.githubusercontent.com/dev-launcher...         webhook-workers   \n4  https://raw.githubusercontent.com/dev-launcher...                strapiv4   \n\n                                         readme_text  \n0  # Dev Launchers (https://devlaunchers.org)\\n\\n...  \n1  # auth-proxy\\nKubernetes dashboard access prox...  \n2  # Onboarding Bot Model\\n\\n## Table of Contents...  \n3  # webhook-workers\\nWebhooks implemented as Clo...  \n4   # Dev Launchers Strapi Service\\n\\n# Getting S...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>url</th>\n      <th>repo_name</th>\n      <th>readme_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://raw.githubusercontent.com/dev-launcher...</td>\n      <td>dev-launchers-platform</td>\n      <td># Dev Launchers (https://devlaunchers.org)\\n\\n...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://raw.githubusercontent.com/dev-launcher...</td>\n      <td>auth-proxy</td>\n      <td># auth-proxy\\nKubernetes dashboard access prox...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://raw.githubusercontent.com/dev-launcher...</td>\n      <td>onboarding-bot-model</td>\n      <td># Onboarding Bot Model\\n\\n## Table of Contents...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://raw.githubusercontent.com/dev-launcher...</td>\n      <td>webhook-workers</td>\n      <td># webhook-workers\\nWebhooks implemented as Clo...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://raw.githubusercontent.com/dev-launcher...</td>\n      <td>strapiv4</td>\n      <td># Dev Launchers Strapi Service\\n\\n# Getting S...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme_files.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 1.1: Pre-process the text, remove special characters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "4b05e160-1451-4a57-bb94-533003ae18c2",
   "metadata": {},
   "source": [
    "## Task 2: Create Documents from the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b5682-3d6a-4954-adbd-67da3c94b6d1",
   "metadata": {},
   "source": [
    "Later in this project, we will be creating vector embeddings for all of the rows in the `movies` DataFrame. Before we do so, we need to create [Document](https://docs.langchain.com/docs/components/schema/document) objects from the data in the DataFrame. To accomplish this, we can utilize the `DataFrameLoader` class provided by langchain, which allows us to create documents from a pandas DataFrame.\n",
    "\n",
    "For the main content of the documents, we will create a summary string that includes relevant information about each movie. To achieve this, we will combine the movie title, description, and genre into a `page_content` column. Additionally, we will retain the IMDB link in the `source` column as metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caf40d3-d1e8-4eb6-ad48-5fa90374b750",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6b483c-f719-4174-be5c-c7589d7de738",
   "metadata": {},
   "source": [
    "- Import `DataFrameLoader` from `langchain.document_loaders`\n",
    "- Create a column `page_content` that creates strings that contain information about the movie title, genre and description. For example, the first movie should look like this:\n",
    "```\n",
    "Title: The Silence of the Lambs\n",
    "Genre: Crime,Drama,Thriller\n",
    "Description: Jodie Foster stars as Clarice Starling, a top student at the FBI's training academy. Jack Crawford (Scott Glenn) wants Clarice to interview Dr. Hannibal Lecter (Anthony Hopkins), a brilliant psychiatrist who is also a violent psychopath, serving life behind bars for various acts of murder and cannibalism. Crawford believes that Lecter may have insight into a case and that Starling, as an attractive young woman, may be just the bait to draw him out.\n",
    "```\n",
    "- Only keep the columns `page_content` and `source` in the movies DataFrame\n",
    "- Use `DataFrameLoader` to load documents from the `movies` DataFrame into `docs`. Use `\"page_content\"` as the `page_content_column`.\n",
    "- Print the first 3 documents and the total number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bca1a148-ca90-4380-8dba-b574068ed108",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1913,
    "lastExecutedAt": 1695209313352,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import DataFrameLoader\nfrom langchain.document_loaders import DataFrameLoader\n\n# Create page content column\nmovies[\"page_content\"] = \"Title: \" + movies[\"movie_title\"] + \"\\n\" + \\\n                         \"Genre: \" + movies[\"genres\"] + \"\\n\" + \\\n                         \"Description: \" +movies[\"movie_description\"]\n\n# Drop all columns except for page_content and source\nmovies = movies[[\"page_content\", \"source\"]]\n\n# Load the documents from the dataframe into docs\n# The page content column is 'movie_description'\ndocs = DataFrameLoader(\n    movies,\n    page_content_column=\"page_content\",\n).load()\n\n# Print the first 3 documents and the number of documents\nprint(f\"First 3 documents: {docs[:3]}\")\nprint(f\"Number of documents: {len(docs)}\")",
    "outputsMetadata": {
     "0": {
      "height": 329,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 documents: [Document(page_content=\"Repo Name: dev-launchers-platform\\nMarkdown Text: # Dev Launchers (https://devlaunchers.org)\\n\\n## We Build People (and software)\\n\\nThis is the monorepo for all DevLaunchers internal products, we use React.js/Next.js and many other wonderful libraries.\\n\\n---\\n\\n## About Dev Launchers\\n\\nDev Launchers is a nonprofit tech company working to democratize access to technology and tech related skills. At our core, we build projects and run programs that are designed to prepare the world for a rapidly changing future. We've built an inclusive, software focused incubator giving people from diverse backgrounds the skills and resources necessary to succeed in careers touched by technology, and have an open community focused on advancing technology, one another, and ourselves. This repository holds the beginnings of the online platform we're creating in order to first impart the general skills required for members to begin their own projects, and then support their exploration as they tackle building something they're excited about. It also functions as our organization's website!\\n\\n---\\n\\n## Contributing\\n\\nVisit https://www.volunteermatch.org/s/srp/orgOpps?org=1189675 to join one of our teams!\\n\\n---\\n\\n## Get Started\\n\\n1. Clone the repo\\n\\n2. Install dependencies: `yarn install`\\n\\n3. Run the tailwindcss compiler: `yarn workspace @devlaunchers/tailwind dev`\\n\\n4. Run the app's development server: `yarn workspace @devlaunchers/app dev`\\n\\n---\\n\\n\\n## Monorepo scripts\\n\\nSome convenience scripts can be run in any folder of this repo and will call their counterparts defined in packages and apps.\\n\\n| Name                         | Description                                                                                                                          |\\n| ---------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |\\n| `yarn g:changeset`           | Add a changeset to declare a new version                                                                                             |\\n| `yarn nuke:node_modules`     | Remove `node_modules` folder in sub-folders                                                                                          |\\n| `yarn commit`                | Format your commit message to follow our convention (Run `git add -p` first)                                                         |\\n| `yarn g:typecheck`           | Run typechecks in all workspaces                                                                                                     |\\n| `yarn g:lint`                | Display linter issues in all workspaces                                                                                              |\\n| `yarn g:lint --fix`          | Attempt to run linter auto-fix in all workspaces                                                                                     |\\n| `yarn g:build`               | Run build in all workspaces                                                                                                          |\\n| `yarn g:clean`               | Clean builds in all workspaces                                                                                                       |\\n| `yarn g:check-dist`          | Ensure build dist files passes es2017 (run `g:build` first). [WIP]                                                                   |\\n| `yarn g:check-size`          | Ensure browser dist files are within size limit (run `g:build` first). [WIP]                                                         |\\n| `yarn clean:global-cache`    | Clean tooling caches (eslint, jest...)                                                                                               |\\n| `yarn deps:check --dep dev`  | Will print what packages can be upgraded globally (see also [.ncurc.yml](https://github.com/sortlist/packages/blob/main/.ncurc.yml)) |\\n| `yarn deps:update --dep dev` | Apply possible updates (run `yarn install && yarn dedupe` after)                                                                     |\\n| `yarn check:install`         | Verify if there's no peer-deps missing in packages                                                                                   |\\n| `yarn dedupe`                | Built-in yarn deduplication of the lock file                                                                                         |\\n\\n> Why using `:` to prefix scripts names ? It's convenient in yarn 3+, we can call those scripts from any folder in the monorepo.\\n> `g:` is a shortcut for `global:`. See the complete list in [root package.json](./package.json).\\n\\n## Maintaining deps updated\\n\\nThe global commands `yarn deps:check` and `yarn deps:update` will help to maintain the same versions across the entire monorepo.\\nThey are based on the excellent [npm-check-updates](https://github.com/raineorshine/npm-check-updates)\\n(see [options](https://github.com/raineorshine/npm-check-updates#options), i.e: `yarn check:deps -t minor`).\\n\\n> After running `yarn deps:update`, a `yarn install` is required. To prevent\\n> having duplicates in the yarn.lock, you can run `yarn dedupe --check` and `yarn dedupe` to\\n> apply deduplication. The duplicate check is enforced in the example github actions.\\n\\n## Release\\n\\nWe are using semantic versioning to tag release. Follow https://github.com/semantic-release/semantic-release#commit-message-format\\nto format the commit messages.\\n\\nWe've created a command to guide you create conventional commit message all you need to do is run `yarn commit`\\n\\nOnce you are ready to create a new release, create a PR to merge master branch to release branch.\\n\\n.\\n\\n---\\n\\n## UI/UX Testing\\n\\n> Available at: https://staging.devlaunchers.org\\n> This runs the main development branch (`master`) and is automatically redeployed when that branch is updated\\n\\n---\\n\\n\\n## Licenses\\n\\nThe Dev Launchers platform is licensed under [GNU General Public License v3](./LICENSE.md).\\n\\nSource: https://raw.githubusercontent.com/dev-launchers/dev-launchers-platform/master/README.md\", metadata={'url': 'https://raw.githubusercontent.com/dev-launchers/dev-launchers-platform/master/README.md'}), Document(page_content='Repo Name: auth-proxy\\nMarkdown Text: # auth-proxy\\nKubernetes dashboard access proxy to map github users to kubernetes users\\n\\nSource: https://raw.githubusercontent.com/dev-launchers/auth-proxy/master/README.md', metadata={'url': 'https://raw.githubusercontent.com/dev-launchers/auth-proxy/master/README.md'}), Document(page_content=\"Repo Name: onboarding-bot-model\\nMarkdown Text: # Onboarding Bot Model\\n\\n## Table of Contents\\n\\n- [Overview](#overview)\\n- [Scenario](#scenario)\\n- [Installation](#installation)\\n- [Usage](#usage)\\n- [Dependencies](#dependencies)\\n- [Files](#files)\\n- [External Help links](#external-help-links)\\n- [License](#license)\\n\\n## Overview\\n\\nThe [Dev Launchers](https://devlaunchers.org) website is a community of aspiring developers eager to learn and gain experience. As such, projects are open-source and ambitious, allowing members to enhance their skills.\\n\\nThe [ChatBot](https://en.wikipedia.org/wiki/Chatbot) project is valuable to this international community working across different time zones. To integrate a new member, it is crucial to be able to address their questions. This becomes challenging when the part of the community capable of responding is located 12 time zones away from the new member. Therefore, it was essential to have a service capable of answering questions, even in the middle of the night.\\n\\n## Scenario\\n\\n1. 🕒 Development of the current [chatbot](https://github.com/dev-launchers/onboarding-bot) by the team, utilizing the [ChatGPT API](https://platform.openai.com/docs/api-reference).\\n2. 🕒 Feasibility study using a list of popular Open Source LLMs of 2023.\\n\\n| 🐳 | 🦜 | Competitor | Statut    | GitHub          | Hugging Face    | Info    |\\n|----|----|------------|-----------|-----------------|-----------------|---------|\\n| ✅ | 🔜 | [Open LLaMA](models/OpenLLaMA/) | Ready       | [Code](https://github.com/openlm-research/open_llama)      | [Model](https://huggingface.co/openlm-research/open_llama_7b) | |\\n| ✅ | ✅ | [Dolly 2.0](models/Dolly_2/)    | Ready       | [Code](https://github.com/databrickslabs/dolly)            | [Model](https://huggingface.co/databricks/dolly-v2-12b) | |\\n| ✅ | 🔜 | [MPT](models/MPT/)              | Ready       | [Code](https://github.com/mosaicml/llm-foundry/)           | [Model](https://huggingface.co/mosaicml/mpt-30b) | |\\n| ✅ | 🔜 | [Alpaca](models/Alpaca/)        | Ready       | [Code](https://github.com/tatsu-lab/stanford_alpaca)       | [Model](https://huggingface.co/tatsu-lab/alpaca-7b-wdiff) | |\\n| ✅ | 🔜 | [FLAN-T5](models/FLAN_T5/)      | Ready       | [Code](https://github.com/lm-sys/FastChat)                 | [Model](https://huggingface.co/google/flan-t5-base) | |\\n| 🚼 | 🔜 | [Llama 2](models/Llama_2/)      | In progress |                                                            | [Model](https://huggingface.co/meta-llama/Llama-2-7b) | |\\n| 🚼 | 🔜 | [Falcon](models/Falcon/)        | In progress |                                                            | [Model](https://huggingface.co/tiiuae/falcon-7b) | |\\n| 🚼 | 🔜 | [Guanaco](models/Guanaco/)      | In progress | [Code](https://github.com/artidoro/qlora/)                 | | |\\n| 🚼 | 🔜 | [Bloom](models/Bloom/)          | In progress | [Code](https://github.com/bigscience-workshop/xmtf#models) | [Model](https://huggingface.co/bigscience/bloom) | |\\n| 🚼 | 🔜 | [GPT NeoXT](models/GPT_NeoXT/)  | In progress | [Code](https://github.com/togethercomputer/OpenChatKit/blob/main/docs/GPT-NeoXT-Chat-Base-20B.md) | [Model](https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B) |         |\\n| 🚼 | 🔜 | [WizardLM](models/WizardLM/)    | In progress | [Code](https://github.com/nlpxucan/WizardLM)               | [Model](https://huggingface.co/WizardLM) | |\\n| 🆘 | 🔜 | [GPT4All](models/GPT4All/)      | Unstartable | [Code](https://github.com/nomic-ai/gpt4all)                | | |\\n| 🆘 | 🔜 | [Mistral](models/Mistral/)      | Unstartable |                                                            | [Model](https://huggingface.co/mistralai) | |\\n|    |    |                                 |             |                                                            | | |\\n\\n3. 📝 Compile and rewrite the readme.md files, procedures, create team descriptions, and consolidate documentation for the tools used.\\n4. 📝 Comparison of results with [MLflow](https://mlflow.org).\\n5. 📝 [Fine-Tuning](https://huggingface.co/docs/transformers/training) of the chosen model(s).\\n6. 📝 Testing within the [Dev Launchers](https://devlaunchers.org) community.\\n\\n## Installation\\n\\nThe chatbots are in their respective folders with an application launcher `autorun.sh` to install each of them without specific knowledge.\\n\\nOn Mac, open the terminal and type:\\n```shell\\ncd\\n```\\nDrag the **`folder`** containing the file `autorun.sh`, then press the Enter key (↩︎).\\n\\n_If you have done it correctly, the **`~`** between your machine's name (`name@MacBook-Pro-of-Name`) and the **`%`** sign should display the name of the `folder` instead._\\n\\nExecute the following line of code by pressing the Enter key (↩︎):\\n```shell\\nsh autorun.sh\\n```\\nWait a moment, the model should open in your default web browser.\\n\\n## Usage\\n\\nYou can try out the different pre-installed language models in the subfolders named after them in the `models` directory.\\n\\n## Dependencies\\n\\n- The project is implemented in `Python 3.11.6`.\\n- All dependencies are include in `requirements.txt` files.\\n\\n### Informations\\n\\nTo obtain the list of installed libraries on the Mac.\\n\\nOn Mac, open the terminal and type:\\n```shell\\ncd\\n```\\nDrag the **`folder`** where you want to create the file `config_mac.txt`, then press the Enter key (↩︎).\\n\\nExecute the following line of code by pressing the Enter key (↩︎):\\n\\n```shell\\npip3 freeze > config_mac.txt\\n```\\n\\n## Files\\n\\n- models\\\\name\\\\FastAPI\\\\\\\\`autorun.sh`: Launch script for download, install and start the application.\\n- models\\\\name\\\\FastAPI\\\\\\\\`Dockerfile`: Contains all the commands a user could call on the command line to assemble the image.\\n- models\\\\name\\\\FastAPI\\\\\\\\`requirements.txt`: The libraries required for the script.\\n- models\\\\name\\\\FastAPI\\\\app\\\\\\\\`main.py`: The script of the web application.\\n\\n## External Help links\\n\\n* [ChatBot Llama](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)\\n    * [ChatBot Llama Exemple](https://llama2.streamlit.app)\\n\\n* [ChatBot Streamlit](https://github.com/streamlit/llm-examples/tree/main)\\n    * [ChatBot Streamlit Exemple](https://llm-examples.streamlit.app)\\n\\n* [ChatGPT API](https://platform.openai.com/docs/introduction)\\n    * [fine-tuning Chat-GPT](https://platform.openai.com/docs/guides/fine-tuning)\\n\\n* LangChain :\\n    * [Hugging Face](https://python.langchain.com/docs/integrations/platforms/huggingface)\\n    * [Hugging Face Tool](https://python.langchain.com/docs/integrations/tools/huggingface_tools)\\n\\n    * [Docs GitHub](https://python.langchain.com/docs/integrations/document_loaders/github)\\n    * [Docs Source Code](https://python.langchain.com/docs/integrations/document_loaders/source_code)\\n\\n    * [subtitles](https://python.langchain.com/docs/integrations/document_loaders/imsdb)\\n\\n## License\\n\\nMIT License\\n\\nCopyright (c) [2023] [Dev Launchers]\\nSource: https://raw.githubusercontent.com/dev-launchers/onboarding-bot-model/main/README.md\", metadata={'url': 'https://raw.githubusercontent.com/dev-launchers/onboarding-bot-model/main/README.md'})]\n",
      "Number of documents: 54\n"
     ]
    }
   ],
   "source": [
    "# Import DataFrameLoader\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "\n",
    "# Create page content column\n",
    "readme_files[\"page_content\"] = \"Repo Name: \" + readme_files[\"repo_name\"] + \"\\n\" + \\\n",
    "                         \"Markdown Text: \" + readme_files[\"readme_text\"] + \"\\n\" + \\\n",
    "                         \"Source: \" + readme_files[\"url\"]\n",
    "\n",
    "# Drop all columns except for page_content and source\n",
    "readme_files = readme_files[[\"page_content\", \"url\"]]\n",
    "\n",
    "# Load the documents from the dataframe into docs\n",
    "docs = DataFrameLoader(\n",
    "    readme_files,\n",
    "    page_content_column=\"page_content\",\n",
    ").load()\n",
    "\n",
    "# Print the first 3 documents and the number of documents\n",
    "print(f\"First 3 documents: {docs[:3]}\")\n",
    "print(f\"Number of documents: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b05e466-807c-42a2-8272-46ba547ae7b6",
   "metadata": {},
   "source": [
    "## Task 3: Estimate the Cost of Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f9087b-d05a-4e4a-a7e7-0e3b13ce8b5d",
   "metadata": {},
   "source": [
    "We're going to be using OpenAI to calculate [vector embeddings](https://platform.openai.com/docs/guides/embeddings/embeddings) of the document texts. Creating embeddings is a form of dimensionality reduction, where we assign the text to a point in an N-dimensional space. Texts that are semantically close to each other should end up being close to each other in the N-dimensional space.\n",
    "\n",
    "Luckily, OpenAI has several models that are trained to calculate these kinds of embeddings, so we don't have to do that ourselves. Of course, a cost is associated with this. You can derive the cost from the [pricing page of OpenAI](https://openai.com/pricing).\n",
    "\n",
    "The calculation is based on the amount of _tokens_ in the text. All text is encoded into tokens to be used by OpenAI. On average, a token consists of roughly 3 characters. However, we can calculate the exact tokens for a string of text by using the `tiktoken` package.\n",
    "\n",
    "The goal of this task is to calculate the number of tokens in the documents, to then extrapolate the estimated cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d29906f-89e9-499e-846b-b69b94920413",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b64d17-7f48-4fe0-90f4-47b70a39e1d4",
   "metadata": {},
   "source": [
    "- Import `tiktoken`\n",
    "- Create the encoder, use the `\"cl100k_base\"` encoder. This is the encoder used by OpenAI to calculate the embeddings for text using the `text-embedding-ada-002` model.\n",
    "- Create a list that contains the amount of tokens for each document\n",
    "- Calculate the estimated cost: the sum of all tokens, divided by 1000 tokens, multiplied with $0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44a1bbe0-b0c5-4295-be95-92711e751755",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1108,
    "lastExecutedAt": 1695209314460,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import tiktoken\nimport tiktoken\n\n# Create the encoder\nencoder = tiktoken.get_encoding(\"cl100k_base\")\n\n# Create a list containing the number of tokens for each document\ntokens_per_doc = [len(encoder.encode(doc.page_content)) for doc in docs]\n\n# Show the estimated cost, which is the sum of the amount of tokens divided by 1000, times $0.0001\ntotal_tokens = sum(tokens_per_doc)\ncost_per_1000_tokens = 0.0001\ncost =  (total_tokens / 1000) * cost_per_1000_tokens\ncost"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.0010829000000000001"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import tiktoken\n",
    "import tiktoken\n",
    "\n",
    "# Create the encoder\n",
    "encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Create a list containing the number of tokens for each document\n",
    "tokens_per_doc = [len(encoder.encode(doc.page_content)) for doc in docs]\n",
    "\n",
    "# Show the estimated cost, which is the sum of the amount of tokens divided by 1000, times $0.0001\n",
    "total_tokens = sum(tokens_per_doc)\n",
    "cost_per_1000_tokens = 0.0001\n",
    "cost =  (total_tokens / 1000) * cost_per_1000_tokens\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c07a82-540e-4c7c-ac04-9426b8511933",
   "metadata": {},
   "source": [
    "## Task 4: Create the Index on Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9084628-ad8d-488f-85b5-230ca5130e3e",
   "metadata": {},
   "source": [
    "Looks like calculating the embeddings is not going to be too expensive. It's always smart to get a rough estimate on the amount of tokens used, so you get an idea of the cost of calculating the embeddings using OpenAI.\n",
    "\n",
    "Now we're ready to create the index on Pinecone. An [index in Pinecone](https://docs.pinecone.io/docs/indexes) can be used to store vectors. You can compare an index in Pinecone to a table in SQL, it stores information of one type of object.\n",
    "\n",
    "In a later task, we'll be creating vectors from the documents we just created using OpenAI's second-generation embedding model. It's important to already know the embeddings we're going to use since we need to know the output dimensions to create an index. For `text-embedding-ada-002`, this is `1536` dimensions ([source](https://platform.openai.com/docs/guides/embeddings/second-generation-models)).\n",
    "\n",
    "At the end of this task, you should be able to find your new index, `imdb-movies`, in the [Pinecone UI](https://app.pinecone.io/).\n",
    "\n",
    "![Pinecone UI](pinecone_ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498035c1-ce3e-4634-8d98-084650953dc3",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df296f65-441a-4b72-8ca0-80d878aa8db2",
   "metadata": {},
   "source": [
    "- Import `os` and `pinecone`\n",
    "- Use `.init` to initialize the Pinecone client with the `\"PINECONE_API_KEY\"` environment variable. Use the `\"gcp-starter\"` environment on Pinecone.\n",
    "- Print all the indexes on Pinecone by using `.list_indexes` on the client.\n",
    "- Use `.create_index` to create an index with the name `\"imdb-movies\"`, but only if it does not exist yet. The metric we'll use is the `\"cosine\"` distance, and as we mentioned above, the embeddings wil have `1536` dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b8841311-9c1b-4b5c-a7f1-6926128de7e4",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 16120,
    "lastExecutedAt": 1695209330580,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import os and pinecone\nimport os\nimport pinecone\n\n# Initialize pinecone using the `PINECONE_API_KEY` variable. Use the gcp-starter environment\npinecone.init(\n    api_key=os.getenv(\"PINECONE_API_KEY\"),\n    environment=\"gcp-starter\"\n)\n\n# Print the indexes\nprint(pinecone.list_indexes())\n\nindex_name = \"imdb-movies\"\n\n# First check that the given index does not exist yet\nif index_name not in pinecone.list_indexes():\n    # Create the 'imbd-movies' index if it does not exist\n    pinecone.create_index(\n        name=index_name,\n        metric='cosine',\n        dimension=1536,\n    )",
    "outputsMetadata": {
     "0": {
      "height": 37,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['readmes-test']\n"
     ]
    }
   ],
   "source": [
    "# Import os and pinecone\n",
    "import os\n",
    "import pinecone\n",
    "\n",
    "# Initialize pinecone using the `PINECONE_API_KEY` variable. Use the gcp-starter environment\n",
    "pinecone.init(\n",
    "    api_key=secrets[\"PINECONE_API_KEY\"],\n",
    "    environment=\"gcp-starter\"\n",
    ")\n",
    "\n",
    "# Print the indexes\n",
    "print(pinecone.list_indexes())\n",
    "\n",
    "index_name = \"readmes-test\"\n",
    "\n",
    "# First check that the given index does not exist yet\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # Create the 'imbd-movies' index if it does not exist\n",
    "    pinecone.create_index(\n",
    "        name=index_name,\n",
    "        metric='cosine',\n",
    "        dimension=1536,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ca370-700a-4fa9-bb85-2ebc49a89308",
   "metadata": {},
   "source": [
    "## Task 5: Fill the Index with the Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea4d375-28be-4deb-a11d-960215c13589",
   "metadata": {},
   "source": [
    "Now that we have the vector index at our disposal, it's time to populate it with some vectors. In this task, we'll need to:\n",
    "\n",
    "1. Generate vector embeddings for all documents in `docs`. We'll utilize OpenAI for this purpose. langchain provides a convenient helper for this task, `langchain.embeddings.openai.OpenAIEmbeddings`, which you can use to generate embeddings using the latest `text-embedding-ada-002` model.\n",
    "2. Populate the vector index in Pinecone with these embeddings. Fortunately, langchain also offers assistance with this through the [`langchain.vectorstores.Pinecone`](https://python.langchain.com/docs/integrations/vectorstores/pinecone) helper.\n",
    "\n",
    "These two steps can be combined using the convenient helper method `.from_document` of the `Pinecone` class. This method accepts an embedding model as input and efficiently calculates the embeddings, subsequently uploading them to Pinecone. We will also introduce some control flow to the code to ensure we do not add data to the Pinecone index if it already contains data. To achieve this, we can make use of the `.from_existing_index` method of `Pinecone`.\n",
    "\n",
    "In addition to storing vectors, Pinecone allows the storage of additional metadata. When using the langchain helpers, it automatically assumes that vectors should be created from the `page_content` property of each `Document`. All other properties will be included as metadata.\n",
    "\n",
    "You can verify that everything has worked correctly by accessing the `imdb-movies` index in the Pinecone UI.\n",
    "\n",
    "![Pinecone UI showing the imdb-movies index](pinecone_ui_index.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290c64c4-c5d2-426f-bb71-2d98fdd2d89c",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1cc456-8a07-4775-892c-729f4b048a86",
   "metadata": {},
   "source": [
    "- Import `OpenAIEmbeddings` from `langchain.embeddings.openai` and `Pinecone` from `langchain.vectorstores` and `Index` from `pinecone.index`\n",
    "- Create the `embeddings` object, which should be an instance of `OpenAIEmbeddings`. The defaults are good to go here.\n",
    "- Use `Pinecone.from_documents` to fill up the vector index on Pinecone using the given documents and embeddings object. This will take a while to run, as it will automatically calculate embeddings from all of the `page_content` properties of the documents, and upload that along with metadata to Pinecone. Assign the result to `docsearch`.\n",
    "   - Some control flow code is already provided for you, this will make sure you use the existing index if it already contains some vectors and avoids filling it up twice.\n",
    "- Test out the `docsearch` vector database object, by calling `.as_retriever().get_relevant_documents` with a given question. This will first create a [retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/) from the vector database, and then use that to match the most similar documents in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2540833a-e503-4717-9dd6-0f90e2c10986",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 32053,
    "lastExecutedAt": 1695209362634,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import OpenAIEmbeddings, Pinecone and Index\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import Pinecone\nfrom pinecone.index import Index\n\n# Create the embeddings object\nembeddings = OpenAIEmbeddings()\n\nindex = Index(index_name)\n\n# Check if there is already some data in the index on Pinecone\nif index.describe_index_stats()['total_vector_count'] > 0:\n    # If there is, use from_existing_index to use the vector store\n    docsearch = Pinecone.from_existing_index(index_name, embeddings)\nelse:\n    # If there is not, use from_documents to fill the vector store\n    docsearch = Pinecone.from_documents(docs, embeddings, index_name=index_name)\n\nquestion = \"What's a good movie about an epic viking?\"\n    \n# Use the vector database as a retriever and get the relevant documents for a quesiton\ndocsearch.as_retriever().get_relevant_documents(question)"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[Document(page_content=\"Repo Name: onboarding-bot-model\\nMarkdown Text: # Onboarding Bot Model\\n\\n## Table of Contents\\n\\n- [Overview](#overview)\\n- [Scenario](#scenario)\\n- [Installation](#installation)\\n- [Usage](#usage)\\n- [Dependencies](#dependencies)\\n- [Files](#files)\\n- [External Help links](#external-help-links)\\n- [License](#license)\\n\\n## Overview\\n\\nThe [Dev Launchers](https://devlaunchers.org) website is a community of aspiring developers eager to learn and gain experience. As such, projects are open-source and ambitious, allowing members to enhance their skills.\\n\\nThe [ChatBot](https://en.wikipedia.org/wiki/Chatbot) project is valuable to this international community working across different time zones. To integrate a new member, it is crucial to be able to address their questions. This becomes challenging when the part of the community capable of responding is located 12 time zones away from the new member. Therefore, it was essential to have a service capable of answering questions, even in the middle of the night.\\n\\n## Scenario\\n\\n1. 🕒 Development of the current [chatbot](https://github.com/dev-launchers/onboarding-bot) by the team, utilizing the [ChatGPT API](https://platform.openai.com/docs/api-reference).\\n2. 🕒 Feasibility study using a list of popular Open Source LLMs of 2023.\\n\\n| 🐳 | 🦜 | Competitor | Statut    | GitHub          | Hugging Face    | Info    |\\n|----|----|------------|-----------|-----------------|-----------------|---------|\\n| ✅ | 🔜 | [Open LLaMA](models/OpenLLaMA/) | Ready       | [Code](https://github.com/openlm-research/open_llama)      | [Model](https://huggingface.co/openlm-research/open_llama_7b) | |\\n| ✅ | ✅ | [Dolly 2.0](models/Dolly_2/)    | Ready       | [Code](https://github.com/databrickslabs/dolly)            | [Model](https://huggingface.co/databricks/dolly-v2-12b) | |\\n| ✅ | 🔜 | [MPT](models/MPT/)              | Ready       | [Code](https://github.com/mosaicml/llm-foundry/)           | [Model](https://huggingface.co/mosaicml/mpt-30b) | |\\n| ✅ | 🔜 | [Alpaca](models/Alpaca/)        | Ready       | [Code](https://github.com/tatsu-lab/stanford_alpaca)       | [Model](https://huggingface.co/tatsu-lab/alpaca-7b-wdiff) | |\\n| ✅ | 🔜 | [FLAN-T5](models/FLAN_T5/)      | Ready       | [Code](https://github.com/lm-sys/FastChat)                 | [Model](https://huggingface.co/google/flan-t5-base) | |\\n| 🚼 | 🔜 | [Llama 2](models/Llama_2/)      | In progress |                                                            | [Model](https://huggingface.co/meta-llama/Llama-2-7b) | |\\n| 🚼 | 🔜 | [Falcon](models/Falcon/)        | In progress |                                                            | [Model](https://huggingface.co/tiiuae/falcon-7b) | |\\n| 🚼 | 🔜 | [Guanaco](models/Guanaco/)      | In progress | [Code](https://github.com/artidoro/qlora/)                 | | |\\n| 🚼 | 🔜 | [Bloom](models/Bloom/)          | In progress | [Code](https://github.com/bigscience-workshop/xmtf#models) | [Model](https://huggingface.co/bigscience/bloom) | |\\n| 🚼 | 🔜 | [GPT NeoXT](models/GPT_NeoXT/)  | In progress | [Code](https://github.com/togethercomputer/OpenChatKit/blob/main/docs/GPT-NeoXT-Chat-Base-20B.md) | [Model](https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B) |         |\\n| 🚼 | 🔜 | [WizardLM](models/WizardLM/)    | In progress | [Code](https://github.com/nlpxucan/WizardLM)               | [Model](https://huggingface.co/WizardLM) | |\\n| 🆘 | 🔜 | [GPT4All](models/GPT4All/)      | Unstartable | [Code](https://github.com/nomic-ai/gpt4all)                | | |\\n| 🆘 | 🔜 | [Mistral](models/Mistral/)      | Unstartable |                                                            | [Model](https://huggingface.co/mistralai) | |\\n|    |    |                                 |             |                                                            | | |\\n\\n3. 📝 Compile and rewrite the readme.md files, procedures, create team descriptions, and consolidate documentation for the tools used.\\n4. 📝 Comparison of results with [MLflow](https://mlflow.org).\\n5. 📝 [Fine-Tuning](https://huggingface.co/docs/transformers/training) of the chosen model(s).\\n6. 📝 Testing within the [Dev Launchers](https://devlaunchers.org) community.\\n\\n## Installation\\n\\nThe chatbots are in their respective folders with an application launcher `autorun.sh` to install each of them without specific knowledge.\\n\\nOn Mac, open the terminal and type:\\n```shell\\ncd\\n```\\nDrag the **`folder`** containing the file `autorun.sh`, then press the Enter key (↩︎).\\n\\n_If you have done it correctly, the **`~`** between your machine's name (`name@MacBook-Pro-of-Name`) and the **`%`** sign should display the name of the `folder` instead._\\n\\nExecute the following line of code by pressing the Enter key (↩︎):\\n```shell\\nsh autorun.sh\\n```\\nWait a moment, the model should open in your default web browser.\\n\\n## Usage\\n\\nYou can try out the different pre-installed language models in the subfolders named after them in the `models` directory.\\n\\n## Dependencies\\n\\n- The project is implemented in `Python 3.11.6`.\\n- All dependencies are include in `requirements.txt` files.\\n\\n### Informations\\n\\nTo obtain the list of installed libraries on the Mac.\\n\\nOn Mac, open the terminal and type:\\n```shell\\ncd\\n```\\nDrag the **`folder`** where you want to create the file `config_mac.txt`, then press the Enter key (↩︎).\\n\\nExecute the following line of code by pressing the Enter key (↩︎):\\n\\n```shell\\npip3 freeze > config_mac.txt\\n```\\n\\n## Files\\n\\n- models\\\\name\\\\FastAPI\\\\\\\\`autorun.sh`: Launch script for download, install and start the application.\\n- models\\\\name\\\\FastAPI\\\\\\\\`Dockerfile`: Contains all the commands a user could call on the command line to assemble the image.\\n- models\\\\name\\\\FastAPI\\\\\\\\`requirements.txt`: The libraries required for the script.\\n- models\\\\name\\\\FastAPI\\\\app\\\\\\\\`main.py`: The script of the web application.\\n\\n## External Help links\\n\\n* [ChatBot Llama](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)\\n    * [ChatBot Llama Exemple](https://llama2.streamlit.app)\\n\\n* [ChatBot Streamlit](https://github.com/streamlit/llm-examples/tree/main)\\n    * [ChatBot Streamlit Exemple](https://llm-examples.streamlit.app)\\n\\n* [ChatGPT API](https://platform.openai.com/docs/introduction)\\n    * [fine-tuning Chat-GPT](https://platform.openai.com/docs/guides/fine-tuning)\\n\\n* LangChain :\\n    * [Hugging Face](https://python.langchain.com/docs/integrations/platforms/huggingface)\\n    * [Hugging Face Tool](https://python.langchain.com/docs/integrations/tools/huggingface_tools)\\n\\n    * [Docs GitHub](https://python.langchain.com/docs/integrations/document_loaders/github)\\n    * [Docs Source Code](https://python.langchain.com/docs/integrations/document_loaders/source_code)\\n\\n    * [subtitles](https://python.langchain.com/docs/integrations/document_loaders/imsdb)\\n\\n## License\\n\\nMIT License\\n\\nCopyright (c) [2023] [Dev Launchers]\\nSource: https://raw.githubusercontent.com/dev-launchers/onboarding-bot-model/main/README.md\", metadata={'url': 'https://raw.githubusercontent.com/dev-launchers/onboarding-bot-model/main/README.md'}),\n Document(page_content='Repo Name: devbots__general\\nMarkdown Text: # Welcome to DevBots!\\n\\n## Please read our [Wiki](https://github.com/dev-launchers/devbots__general/wiki) for information!\\n\\n![Logo](https://github.com/dev-launchers/devbots__general/blob/main/art/Logos/LOGO-DevBots_dk_bk.jpg \"Logo\")\\n\\nAll code created by any party for this project falls under our [LICENSE](https://github.com/dev-launchers/devbots__general/blob/main/LICENSE).\\n\\nSource: https://raw.githubusercontent.com/dev-launchers/devbots__general/main/README.md', metadata={'url': 'https://raw.githubusercontent.com/dev-launchers/devbots__general/main/README.md'}),\n Document(page_content='Repo Name: devbots__backend\\nMarkdown Text: # devbots__backend\\nThe backend logic and systems for DevBots \\nCheck out our [wiki](https://github.com/dev-launchers/devbots__general/wiki)!\\n[API Documentation](https://studio-ws.apicur.io/sharing/55d30746-025f-4fd8-9b13-eebc86543001)\\n\\n## Code Style\\n\\nGeneral Coding Style will follow the Framework of [Pragmatic Functional Java](https://github.com/siy/pragmatica/wiki). ([Further Read](https://dev.to/siy/introduction-to-pragmatic-functional-java-142m))\\n\\nSource: https://raw.githubusercontent.com/dev-launchers/devbots__backend/main/README.md', metadata={'url': 'https://raw.githubusercontent.com/dev-launchers/devbots__backend/main/README.md'}),\n Document(page_content='Repo Name: devbots__game\\nMarkdown Text: # blockchain__game\\n## Please read [our wiki](https://github.com/dev-launchers/devbots__general/wiki) before contributing!\\n\\nSource: https://raw.githubusercontent.com/dev-launchers/devbots__game/main/README.md', metadata={'url': 'https://raw.githubusercontent.com/dev-launchers/devbots__game/main/README.md'})]"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import OpenAIEmbeddings, Pinecone and Index\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from pinecone.index import Index\n",
    "\n",
    "# Create the embeddings object\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=secrets[\"OPENAI_API_KEY\"])\n",
    "\n",
    "index = Index(index_name)\n",
    "\n",
    "# Check if there is already some data in the index on Pinecone\n",
    "if index.describe_index_stats()['total_vector_count'] > 0:\n",
    "    # If there is, use from_existing_index to use the vector store\n",
    "    docsearch = Pinecone.from_existing_index(index_name, embeddings)\n",
    "else:\n",
    "    # If there is not, use from_documents to fill the vector store\n",
    "    docsearch = Pinecone.from_documents(docs, embeddings, index_name=index_name)\n",
    "\n",
    "question = \"What is the onboarding bot project?\"\n",
    "    \n",
    "# Use the vector database as a retriever and get the relevant documents for a quesiton\n",
    "docsearch.as_retriever().get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "116f1dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Document(page_content=\"Repo Name: minecraft__pack\\nMarkdown Text: # Want to help us with the resource pack?? Download the .zip file!\\n\\n---\\n\\n## How to download this pack\\n### Download the current version of the asset pack from __this__ page (the one you're on right now!)\\n### Follow the two simple steps in the image below:\\n[![](https://cdn.discordapp.com/attachments/705526685134487614/785335049695658004/download_1.png)](Screenshot)\\n\\n\\n### Then, you're free to change the corresponding files in the pack!\\n\\n---\\n\\n#### Need help?\\nCheck https://minecraft.gamepedia.com/Tutorials/Creating_a_resource_pack for more info on how to work with Minecraft resource packs\\n\\n## What is this repo?\\nThis repo is a place to store the texture-pack for the Dev Launchers Minecraft Server.\\nFeel free to leave your thoughts/suggestions, contribute changes and cool new versions of the assets, and play with the pack!\\n\\n\\nSource: https://raw.githubusercontent.com/dev-launchers/minecraft__pack/main/README.md\", metadata={'url': 'https://raw.githubusercontent.com/dev-launchers/minecraft__pack/main/README.md'})"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What learning resources can I get?\"\n",
    "docsearch.as_retriever().get_relevant_documents(question)[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de4edfc-55fa-42ec-a23d-3d9607216e2e",
   "metadata": {},
   "source": [
    "## Task 6: Create Prompts for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aa737c-4947-48d4-a9e9-4d5cd38b21c0",
   "metadata": {},
   "source": [
    "In the previous task, we observed that the vector store can be utilized to retrieve relevant documents related to specific queries. For instance, when asked \"What's a good movie about vikings?\", the movie 'The Northman' was returned as a result. It is important to note that we did not incorporate any measure of movie quality into the system, so the notion of the movie being \"good\" is not explicitly encoded in the embeddings. It is crucial to always consider the data provided to the system and interpret the results of the AI system within that context. To enhance the results, one approach could be to include information about the movie quality in the movie description.\n",
    "\n",
    "The remarkable aspect of RAG is the ability to provide relevant context to the LLM within the prompt itself. In the aforementioned example, we would include a description of 'The Northman' in the prompt, enabling the LLM to generate factual information beyond its knowledge cutoff. 'The Northman' was released in 2022, while the knowledge cutoff for the `gpt-3.5-turbo` model is set at September 2021.\n",
    "\n",
    "Now that you understand how the retriever can be employed to retrieve relevant documents from the vector database, we need to devise a prompt that presents this information to the LLM when we pose a question.\n",
    "\n",
    "We require two types of [prompt templates](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/):\n",
    "- A template that demonstrates how the information in relevant documents is presented to the LLM\n",
    "- A template that combines the context with the rest of the prompt\n",
    "\n",
    "Some example prompt templates are provided in the sample code below, which you are free to edit. Notice that these example templates contain `=========` separators between different parts of the text. These kinds of delimiters are a common tactic to help the LLM distinguish between different parts of your input prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e31746-7fd7-4a2c-a658-d1640716e930",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73889c7-4776-4290-abe1-9bb272665c79",
   "metadata": {},
   "source": [
    "- Import `PromptTemplate` from `langchain.prompts`\n",
    "- Some example prompt templates are already provided for you. You are free to adapt them at your will. There are two prompt templates:\n",
    "  - `DOCUMENT_PROMPT`: this template shows how a summary text is created for each document. The properties between the curly brackets (`{`) are replaced with the properties of each `Document`.\n",
    "  - `QUESTION_PROMPT`: this template creates the full prompt that is sent to the LLM. `question` is replaced by the question asked by the user, and `summaries` is replaced with the summary of each relevant document, created by the `DOCUMENT_PROMPT` template\n",
    "- Create the `PromptTemplate` objects by using `PromptTemplate.from_template`. Call them `document_prompt` and `question_prompt`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "45d251c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\"Repo Name: dev-launchers-platform\\nMarkdown Text: # Dev Launchers (https://devlaunchers.org)\\n\\n## We Build People (and software)\\n\\nThis is the monorepo for all DevLaunchers internal products, we use React.js/Next.js and many other wonderful libraries.\\n\\n---\\n\\n## About Dev Launchers\\n\\nDev Launchers is a nonprofit tech company working to democratize access to technology and tech related skills. At our core, we build projects and run programs that are designed to prepare the world for a rapidly changing future. We've built an inclusive, software focused incubator giving people from diverse backgrounds the skills and resources necessary to succeed in careers touched by technology, and have an open community focused on advancing technology, one another, and ourselves. This repository holds the beginnings of the online platform we're creating in order to first impart the general skills required for members to begin their own projects, and then support their exploration as they tackle building something they're excited about. It also functions as our organization's website!\\n\\n---\\n\\n## Contributing\\n\\nVisit https://www.volunteermatch.org/s/srp/orgOpps?org=1189675 to join one of our teams!\\n\\n---\\n\\n## Get Started\\n\\n1. Clone the repo\\n\\n2. Install dependencies: `yarn install`\\n\\n3. Run the tailwindcss compiler: `yarn workspace @devlaunchers/tailwind dev`\\n\\n4. Run the app's development server: `yarn workspace @devlaunchers/app dev`\\n\\n---\\n\\n\\n## Monorepo scripts\\n\\nSome convenience scripts can be run in any folder of this repo and will call their counterparts defined in packages and apps.\\n\\n| Name                         | Description                                                                                                                          |\\n| ---------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |\\n| `yarn g:changeset`           | Add a changeset to declare a new version                                                                                             |\\n| `yarn nuke:node_modules`     | Remove `node_modules` folder in sub-folders                                                                                          |\\n| `yarn commit`                | Format your commit message to follow our convention (Run `git add -p` first)                                                         |\\n| `yarn g:typecheck`           | Run typechecks in all workspaces                                                                                                     |\\n| `yarn g:lint`                | Display linter issues in all workspaces                                                                                              |\\n| `yarn g:lint --fix`          | Attempt to run linter auto-fix in all workspaces                                                                                     |\\n| `yarn g:build`               | Run build in all workspaces                                                                                                          |\\n| `yarn g:clean`               | Clean builds in all workspaces                                                                                                       |\\n| `yarn g:check-dist`          | Ensure build dist files passes es2017 (run `g:build` first). [WIP]                                                                   |\\n| `yarn g:check-size`          | Ensure browser dist files are within size limit (run `g:build` first). [WIP]                                                         |\\n| `yarn clean:global-cache`    | Clean tooling caches (eslint, jest...)                                                                                               |\\n| `yarn deps:check --dep dev`  | Will print what packages can be upgraded globally (see also [.ncurc.yml](https://github.com/sortlist/packages/blob/main/.ncurc.yml)) |\\n| `yarn deps:update --dep dev` | Apply possible updates (run `yarn install && yarn dedupe` after)                                                                     |\\n| `yarn check:install`         | Verify if there's no peer-deps missing in packages                                                                                   |\\n| `yarn dedupe`                | Built-in yarn deduplication of the lock file                                                                                         |\\n\\n> Why using `:` to prefix scripts names ? It's convenient in yarn 3+, we can call those scripts from any folder in the monorepo.\\n> `g:` is a shortcut for `global:`. See the complete list in [root package.json](./package.json).\\n\\n## Maintaining deps updated\\n\\nThe global commands `yarn deps:check` and `yarn deps:update` will help to maintain the same versions across the entire monorepo.\\nThey are based on the excellent [npm-check-updates](https://github.com/raineorshine/npm-check-updates)\\n(see [options](https://github.com/raineorshine/npm-check-updates#options), i.e: `yarn check:deps -t minor`).\\n\\n> After running `yarn deps:update`, a `yarn install` is required. To prevent\\n> having duplicates in the yarn.lock, you can run `yarn dedupe --check` and `yarn dedupe` to\\n> apply deduplication. The duplicate check is enforced in the example github actions.\\n\\n## Release\\n\\nWe are using semantic versioning to tag release. Follow https://github.com/semantic-release/semantic-release#commit-message-format\\nto format the commit messages.\\n\\nWe've created a command to guide you create conventional commit message all you need to do is run `yarn commit`\\n\\nOnce you are ready to create a new release, create a PR to merge master branch to release branch.\\n\\n.\\n\\n---\\n\\n## UI/UX Testing\\n\\n> Available at: https://staging.devlaunchers.org\\n> This runs the main development branch (`master`) and is automatically redeployed when that branch is updated\\n\\n---\\n\\n\\n## Licenses\\n\\nThe Dev Launchers platform is licensed under [GNU General Public License v3](./LICENSE.md).\\n\\nSource: https://raw.githubusercontent.com/dev-launchers/dev-launchers-platform/master/README.md\""
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme_files.iloc[0][\"page_content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9b2932a2-8b1b-48d6-a18d-bc2f12ebc2c5",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1695209362686,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import PromptTemplate\nfrom langchain.prompts import PromptTemplate\n\n# Read/adapt the prompts below at will\nDOCUMENT_PROMPT = \"\"\"{page_content}\nIMDB link: {source}\n=========\"\"\"\n\nQUESTION_PROMPT = \"\"\"Given the following extracted parts of a movie database and a question, create a final answer with the IMDB link as source (\"SOURCE\").\nIf you don't know the answer, just say that you don't know. Don't try to make up an answer.\nALWAYS return a \"SOURCE\" part in your answer.\n\nQUESTION: What's a good movie about a robot to watch with my kid?\n=========\nTitle: A.I. Artificial Intelligence\nGenre: Drama,Sci-Fi\nDescription: A robotic boy, the first programmed to love, David (Haley Joel Osment) is adopted as a test case by a Cybertronics employee (Sam Robards) and his wife (Frances O'Connor). Though he gradually becomes their child, a series of unexpected circumstances make this life impossible for David. Without final acceptance by humans or machines, David embarks on a journey to discover where he truly belongs, uncovering a world in which the line between robot and machine is both vast and profoundly thin.\nIMDB link: https://www.imdb.com/title/tt0212720\n=========\nTitle: I, Robot\nGenre: Action,Mystery,Sci-Fi\nDescription: In 2035, highly intelligent robots fill public service positions throughout the world, operating under three rules to keep humans safe. Despite his dark history with robotics, Detective Del Spooner (Will Smith) investigates the alleged suicide of U.S. Robotics founder Alfred Lanning (James Cromwell) and believes that a human-like robot (Alan Tudyk) murdered him. With the help of a robot expert (Bridget Moynahan), Spooner discovers a conspiracy that may enslave the human race.\nIMDB link: https://www.imdb.com/title/tt0343818\n=========\nTitle: The Iron Giant\nGenre: Action,Adventure,Animation\nDescription: In this animated adaptation of Ted Hughes' Cold War fable, a giant alien robot (Vin Diesel) crash-lands near the small town of Rockwell, Maine, in 1957. Exploring the area, a local 9-year-old boy, Hogarth, discovers the robot, and soon forms an unlikely friendship with him. When a paranoid government agent, Kent Mansley, becomes determined to destroy the robot, Hogarth and beatnik Dean McCoppin (Harry Connick Jr.) must do what they can to save the misunderstood machine.\nIMDB link: https://www.imdb.com/title/tt0129167\n=========\nFINAL ANSWER: 'The Iron Giant' is an animated movie about a friendship between a robot and a kid. It would be a good movie to watch with a kid.\nSOURCE: https://www.imdb.com/title/tt0129167\n\nQUESTION: {question}\n=========\n{summaries}\nFINAL ANSWER:\"\"\"\n\n# Create prompt template objects\ndocument_prompt = PromptTemplate.from_template(DOCUMENT_PROMPT)\nquestion_prompt = PromptTemplate.from_template(QUESTION_PROMPT)"
   },
   "outputs": [],
   "source": [
    "# Import PromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Read/adapt the prompts below at will\n",
    "DOCUMENT_PROMPT = \"\"\"{page_content}\n",
    "=========\"\"\"\n",
    "\n",
    "QUESTION_PROMPT = \"\"\"Given the following extracted parts, create a final answer with the text of the readme file.\n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "Be concise and do not add additional text that does not exist in the note.\n",
    "Use the Markdown Text to actively answer the question.\n",
    "If there's not a single repo that clearly answers the question, Just provide the URL of the potential useful ones and NOT access their Markdown Text.\n",
    "The source should be the URL of the readme file or repository.\n",
    "\n",
    "QUESTION: What learning resources can I get?\n",
    "FINAL ANSWER: Learning resources can be found in the learning resource repository, for example:\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{summaries}\n",
    "FINAL ANSWER:\"\"\"\n",
    "\n",
    "# Create prompt template objects\n",
    "document_prompt = PromptTemplate.from_template(DOCUMENT_PROMPT)\n",
    "question_prompt = PromptTemplate.from_template(QUESTION_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f5e246-7f71-4d1e-ac5a-c555e37f8849",
   "metadata": {},
   "source": [
    "## Task 7: Chain Everything Together to Perform RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1975eeaf-23d0-4d71-a525-01ab10b64096",
   "metadata": {},
   "source": [
    "Finally, we have the vector index filled up with information, we have the prompt templates set up. That means we have everything we need to build a question-answering bot, which can use the information retrieved from Pinecone to answer questions about movies.\n",
    "\n",
    "We'll use the `gpt-3.5-turbo` model of OpenAI in order to provide a completion for the question prompt above.\n",
    "\n",
    "Langchain provides a convenient concept, called [chains](https://python.langchain.com/docs/modules/chains/), that does some of the heavy lifting when you need to combine multiple AI systems into a single application. For the purpose of this project, we'll be using the `RetrievalQAWithSourcesChain` class. This chain will accept a `question` and a `retriever`. When asked a question, it will first use the retriever to retrieve relevant documents. Afterwards, it will combine the documents into a prompt and send it to the LLM to provide a completion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9006ac2b-f0a2-4d0a-bd50-133a39f58d54",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648165d9-66a0-4aa6-aae2-57f0136d89ad",
   "metadata": {},
   "source": [
    "- Import `RetrievalQAWithSourcesChain` from `langchain.chains` and `ChatOpenAI` from `langchain.chat_models`\n",
    "- Use `RetrievalQAWithSourcesChain` to create the chain to answer questions. Use the `.from_chain_type` method:\n",
    "  - Set `chain_type` set to `\"stuff\"`. This is the simplest type of chain, and will just stuff the document context in one prompt.\n",
    "  - Set `llm` to an instance of `ChatOpenAI` with `model_name` set to `\"gpt-3.5-turbo\"` and `temperature` set to `0`.\n",
    "  - Use the `PromptTemplate` objects you created above to pass to `chain_type_kwargs`\n",
    "  - As a retriever, use the `docsearch.as_retriever` method you've seen before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c5382d5f-6813-41d3-9f55-7e404c050c76",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 2750,
    "lastExecutedAt": 1695209365436,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import RetrievalQAWithSourcesChain and ChatOpenAI\nfrom langchain.chains import RetrievalQAWithSourcesChain\nfrom langchain.chat_models import ChatOpenAI\n\n# Create the QA bot LLM chain\nqa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n    chain_type=\"stuff\",\n    llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0),\n    chain_type_kwargs={\n        \"document_prompt\": document_prompt,\n        \"prompt\": question_prompt,\n    },\n    retriever=docsearch.as_retriever(),\n)\n\n# Ask the LLM a question about movies\nqa_with_sources(question)",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import RetrievalQAWithSourcesChain and ChatOpenAI\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Create the QA bot LLM chain\n",
    "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    chain_type=\"stuff\",\n",
    "    llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=secrets[\"OPENAI_API_KEY\"]),\n",
    "    chain_type_kwargs={\n",
    "        \"document_prompt\": document_prompt,\n",
    "        \"prompt\": question_prompt,\n",
    "    },\n",
    "    retriever=docsearch.as_retriever( search_kwargs={'k': 6}),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "46242764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'question': 'What are the models tested in onboarding bot project?',\n 'answer': 'The models tested in the onboarding bot project are:\\n\\n- Open LLaMA\\n- Dolly 2.0\\n- MPT\\n- Alpaca\\n- FLAN-T5\\n- Llama 2\\n- Falcon\\n- Guanaco\\n- Bloom\\n- GPT NeoXT\\n- WizardLM\\n- GPT4All\\n- Mistral\\n\\n',\n 'sources': '[onboarding-bot-model](https://raw.githubusercontent.com/dev-launchers/onboarding-bot-model/main/README.md)'}"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the models tested in onboarding bot project?\"\n",
    "qa_with_sources(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e1650cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'question': 'Where can I find installation help for the platform development environment?',\n 'answer': \"For installation help for the platform development environment, you can refer to the Dev Launchers platform dev env repository. The repository provides a docker-compose file to run the api, bot, and database to emulate the backend environment. It also includes prerequisites such as Docker and Tilt, and provides setup and running instructions. You can find more information in the repository's [README](https://raw.githubusercontent.com/dev-launchers/platform__dev-env/main/README.md).\\n\\n\",\n 'sources': '[platform__dev-env](https://raw.githubusercontent.com/dev-launchers/platform__dev-env/main/README.md)'}"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Where can I find installation help for the platform development environment?\"\n",
    "qa_with_sources(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "{'question': 'What is dev launchers platform?',\n 'answer': 'Dev Launchers is a platform that provides various services and projects for developers. Here are some details about the different repositories:\\n\\n1. [Strapi](https://raw.githubusercontent.com/dev-launchers/strapi/main/README.md): This repository contains the Dev Launchers Strapi Service. To contribute, follow the contributing guide. The service has staging and production URLs.\\n\\n2. [Platform Dev Env](https://raw.githubusercontent.com/dev-launchers/platform__dev-env/main/README.md): This repository hosts the docker-compose file to run the API, bot, and database to emulate the backend environment. It requires Docker and Tilt for setup. The repository provides instructions for running and collaborating on the dev environment.\\n\\n3. [Platform DL-Edu](https://raw.githubusercontent.com/dev-launchers/platform__dl-edu/main/README.md): This repository is for the Dev Launchers Edu Project. To run the project, navigate to the root directory and install the dependencies using npm. The project will be available at http://localhost:3000.\\n\\n4. [DevBots Backend](https://raw.githubusercontent.com/dev-launchers/devbots__backend/main/README.md): This repository contains the backend logic and systems for DevBots. Check out the wiki for more information and the API documentation.\\n\\n5. [Strapiv4](https://raw.githubusercontent.com/dev-launchers/strapiv4/main/README.md): This repository is another version of the Dev Launchers Strapi Service. It provides instructions for getting started, running from Docker, contributing, and updating the Strapi version.\\n\\n6. [DevBots General](https://raw.githubusercontent.com/dev-launchers/devbots__general/main/README.md): This repository is the general repository for DevBots. It provides a wiki for information and has a license for the code.\\n\\nFor more details, please refer to the respective repositories.',\n 'sources': ''}"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is dev launchers platform?\"\n",
    "qa_with_sources(question)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "Document(page_content=\"Repo Name: strapiv4\\nMarkdown Text:  # Dev Launchers Strapi Service\\n\\n# Getting Started\\n1. Copy the `.env.example` file into `.env`\\n2. Run `npm install`\\n3. Run `npm run develop`\\n4. Go to http://localhost:1337/admin to create an account\\n\\n# Running from Docker\\nAlternatively, you can run it with Docker. There are 2 make targets available to do this.\\n- Ensure that Docker is running. This usually means that you need to start up Docker Desktop.\\n- cd to the project's root directory\\n- `make build-docker` to build the docker container. This may take a minute.\\n- `make run-docker` to start up the strapiv4 server.\\n\\n### **_Note_**: You do not need to do the build and run steps every time you make a change.\\nOnce the docker container is running, the strapi server will auto-reload with your changes without having to restart the container.\\n\\nAlso, if the container goes down for some reason, you do not need to rebuild it again. Run `make run-docker` and it will start up again.\\n\\n# Contributing\\nFollow the [contributing guide](./CONTRIBUTING.md)\\n\\n# Release\\nWe are using semantic versioning to tag release. Follow https://github.com/semantic-release/semantic-release#commit-message-format\\nto format the commit messages.\\n\\nOnce you are ready to create a new release, create a PR to merge main branch to release branch.\\n\\n# Update Strapi Version\\nGo to https://github.com/strapi/strapi/releases to find the latest version, then update all `@strapi` packages in\\n`package.json` to this version and run `npm install`.\\n\\n# Integration Test\\nTo run the integration test envrionment, set `NODE_ENV=test` in `.env` file. The login email is `integration-test@devlaunchers.org`\\npassword is `wQ46^BEsVbf9VD&D3ddSPCUP`. Data is stored in `integration-test/data.db`.\\nSource: https://raw.githubusercontent.com/dev-launchers/strapiv4/main/README.md\", metadata={'url': 'https://raw.githubusercontent.com/dev-launchers/strapiv4/main/README.md'})"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch.as_retriever().get_relevant_documents(question)[3]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "7f6c51c7-bbbb-4f0f-b218-850221f3dcdf",
   "metadata": {},
   "source": [
    "## Task 8: Add Debug Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce00845-9741-4979-91aa-e43ff0db5299",
   "metadata": {},
   "source": [
    "Let's take a moment to address what we've achieved by using RAG, which would be impossible to achieve with just using `gpt-3.5-turbo` as an LLM:\n",
    "\n",
    "1. We enabled the LLM to answer the question with factual information, which can even be information from after ChatGPT's knowledge cutoff (which is September 2021).\n",
    "2. We enabled the LLM to provide sources with the answer it generates.\n",
    "\n",
    "Pretty neat, right?\n",
    "\n",
    "We saw that langchain is very convenient when it comes to quickly creating smart AI systems. However, for learning, it can be quite challenging to understand what's happening behind the scenes. For example, from the code in Task 7, it's not clear that `qa_with_sources` actually first calls Pinecone to retrieve documents, then uses those documents to fill in the prompt to send along to the `gpt-3.5-turbo` LLM.\n",
    "\n",
    "Let's look at how we can get some more insights into how this all works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b998f52-93c9-411c-ba74-473955de4b8e",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19bfda1-fe9a-4344-9dd9-613d33598d21",
   "metadata": {},
   "source": [
    "- Import `langchain`\n",
    "- Set `.debug` to `True` on `langchain`\n",
    "- Run `qa_with_sources(question)` again\n",
    "\n",
    "Observe the information that is printed in the output. Langchain enables you to run chains of LLMs or other AI systems, one after the other. The input for the next chain is passed on from the previous, where new information can be added by, for example, using embeddings to find relevant documents. Each chain or LLM is marked with a tag like `[chain/start]` or `[llm/start]`. When a final response is fetched from the last part of the chain, the output travels back up the chain. This is marked with the `[chain/end]` and `[llm/end]` marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "474d98a8-33ce-4898-a641-4853f17e5738",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 2606,
    "lastExecutedAt": 1695209368042,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import langchain\nimport langchain\n\n# Enable debug logging\nlangchain.debug = True\n\n# Ask the LLM a question about movies\nqa_with_sources(question)",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'question': 'What is dev launchers platform?',\n 'answer': 'Dev Launchers is a platform that provides various services and projects for developers. Here are some details about the different repositories:\\n\\n1. [Strapi](https://raw.githubusercontent.com/dev-launchers/strapi/main/README.md): This repository contains the Dev Launchers Strapi Service. To contribute, follow the contributing guide. The service has staging and production URLs.\\n\\n2. [Platform Dev Env](https://raw.githubusercontent.com/dev-launchers/platform__dev-env/main/README.md): This repository hosts the docker-compose file to run the API, bot, and database to emulate the backend environment. It requires Docker and Tilt for setup. The repository provides instructions for running and collaborating on the dev environment.\\n\\n3. [Platform DL-Edu](https://raw.githubusercontent.com/dev-launchers/platform__dl-edu/main/README.md): This repository is for the Dev Launchers Edu Project. To run the project, navigate to the root directory and install dependencies using npm. The project will be available at http://localhost:3000.\\n\\n4. [DevBots Backend](https://raw.githubusercontent.com/dev-launchers/devbots__backend/main/README.md): This repository contains the backend logic and systems for DevBots. Check out the wiki for more information and the API documentation.\\n\\n5. [Strapiv4](https://raw.githubusercontent.com/dev-launchers/strapiv4/main/README.md): This repository is another version of the Dev Launchers Strapi Service. It provides instructions for getting started, running from Docker, contributing, and updating the Strapi version.\\n\\n6. [DevBots General](https://raw.githubusercontent.com/dev-launchers/devbots__general/main/README.md): This repository is the general repository for DevBots. It provides a wiki for information and has a license for the code.\\n\\nFor more details, please refer to the respective repositories.',\n 'sources': ''}"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import langchain\n",
    "import langchain\n",
    "\n",
    "# Enable debug logging\n",
    "langchain.debug = True\n",
    "\n",
    "# Ask the LLM a question about movies\n",
    "qa_with_sources(question)"
   ]
  }
 ],
 "metadata": {
  "editor": "DataCamp Workspace",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
