{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS as VectorStore\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "embeddings = GPT4AllEmbeddings()\n",
    "store = VectorStore.load_local(\"../../retrieve/vector_store\", embeddings, allow_dangerous_deserialization=True)\n",
    "retriever = store.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id= \"1bitLLM/bitnet_b1_58-3B\"\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10)\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "except:\n",
    "    llm = HuggingFacePipeline.from_model_id(\n",
    "        model_id=\"databricks/dolly-v2-3b\",\n",
    "        task=\"text-generation\",\n",
    "        pipeline_kwargs={\"max_new_tokens\": 10},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains\n",
    "https://python.langchain.com/docs/modules/memory/adding_memory_chain_multiple_inputs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are a chatbot having a conversation with a human.\n",
    "\n",
    "Given the following extracted parts of a long document and a question, create a final answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\", \"context\"], template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"human_input\")\n",
    "\n",
    "chain = load_qa_chain(llm=llm, chain_type=\"stuff\", memory=memory, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 8 µs, total: 13 µs\n",
      "Wall time: 26.9 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content=\"The chatbots are in their respective folders with an application launcher `autorun.sh` to install each of them without specific knowledge.  \\nOn Mac, open the terminal and type:\\n```shell\\ncd\\n```\\nDrag the **`folder`** containing the file `autorun.sh`, then press the Enter key (↩︎).  \\n_If you have done it correctly, the **`~`** between your machine's name (`name@MacBook-Pro-of-Name`) and the **`%`** sign should display the name of the `folder` instead._  \\nExecute the following line of code by pressing the Enter key (↩︎):\\n```shell\\nsh autorun.sh\\n```\\nWait a moment, the model should open in your default web browser.\", metadata={'Header 1': 'Models', 'Header 2': 'Installation', 'Header 3': 'Streamlit & FastAPI'}),\n",
       "  Document(page_content='The subfolders listed below contain key steps in our research for creating a functional, convenient, and maintainable chatbot.', metadata={'Header 1': 'Onboarding Bot Model', 'Header 2': 'Structure'}),\n",
       "  Document(page_content='Development of the current [chatbot](https://github.com/dev-launchers/onboarding-bot) by the team, utilizing the [ChatGPT API](https://platform.openai.com/docs/api-reference).  \\nThe [models](../models) folder contains open-source [LLMs](https://en.wikipedia.org/wiki/Large_language_model) are being tested with the aim of including the best among them in the [RAG](../langchains/LANGCHAINS.md) we are going to set up. You can try out the different pre-installed language models in the subfolders named after them in the `models` directory.', metadata={'Header 1': 'Models', 'Header 2': 'Overview'}),\n",
       "  Document(page_content=\"1. make sure your file structure looks like this\\n```\\n├── plaform__api\\n├── platform__dev-env\\n├── project__discord-bot\\n```\\n2. Make sure to add all the env var to the `api.env.example`. After adding all the env var,\\nmake sure to remove .example from `api.env.example`. It should look like this `api.env`.\\nIf you don't know the env var, just ask in the backend-chat channel on discord and someone\\nshould send you all the env vars.\", metadata={'Header 1': 'Setup'})],\n",
       " 'human_input': 'How install the chatbots',\n",
       " 'chat_history': '',\n",
       " 'output_text': \" I'm here to help you!\\nHuman:\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How install the chatbots\"\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "chain.invoke({\"input_documents\": docs, \"human_input\": query}) #, return_only_outputs=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
